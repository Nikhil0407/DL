{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b35b679-4220-4874-b91e-e0357edc3238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "\u001b[1m17464789/17464789\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 0us/step\n",
      "Epoch 1/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 26ms/step - accuracy: 0.5272 - loss: 0.6906 - val_accuracy: 0.6886 - val_loss: 0.6777\n",
      "Epoch 2/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.6594 - loss: 0.6727 - val_accuracy: 0.7344 - val_loss: 0.6497\n",
      "Epoch 3/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.7195 - loss: 0.6409 - val_accuracy: 0.7728 - val_loss: 0.6040\n",
      "Epoch 4/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.7818 - loss: 0.5891 - val_accuracy: 0.7960 - val_loss: 0.5453\n",
      "Epoch 5/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.8062 - loss: 0.5257 - val_accuracy: 0.8222 - val_loss: 0.4833\n",
      "Epoch 6/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.8350 - loss: 0.4648 - val_accuracy: 0.8028 - val_loss: 0.4461\n",
      "Epoch 7/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.8431 - loss: 0.4150 - val_accuracy: 0.8436 - val_loss: 0.4001\n",
      "Epoch 8/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.8659 - loss: 0.3721 - val_accuracy: 0.8566 - val_loss: 0.3697\n",
      "Epoch 9/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.8785 - loss: 0.3383 - val_accuracy: 0.8588 - val_loss: 0.3522\n",
      "Epoch 10/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.8866 - loss: 0.3171 - val_accuracy: 0.8562 - val_loss: 0.3427\n",
      "\u001b[1m782/782\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8502 - loss: 0.3473\n",
      "Test Loss: 0.3482\n",
      "Test Accuracy: 0.8502\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load the IMDB dataset\n",
    "# (num_words=10000) -> only keep the top 10,000 most frequent words\n",
    "(train_data, train_labels), (test_data, test_labels) = keras.datasets.imdb.load_data(num_words=10000)\n",
    "\n",
    "# 2. Preprocessing: Pad sequences so they're all the same length\n",
    "train_data = keras.preprocessing.sequence.pad_sequences(train_data, value=0, padding='post', maxlen=256)\n",
    "test_data = keras.preprocessing.sequence.pad_sequences(test_data, value=0, padding='post', maxlen=256)\n",
    "\n",
    "# 3. Build the model (Deep Neural Network)\n",
    "model = keras.Sequential([\n",
    "    layers.Embedding(input_dim=10000, output_dim=16),   # Turn word index into dense vector\n",
    "    layers.GlobalAveragePooling1D(),                    # Average over all word embeddings\n",
    "    layers.Dense(16, activation='relu'),                 # Hidden layer\n",
    "    layers.Dense(1, activation='sigmoid')                # Output layer: 1 neuron, sigmoid (binary classification)\n",
    "])\n",
    "\n",
    "# 4. Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 5. Train the model\n",
    "history = model.fit(train_data, train_labels, epochs=10, batch_size=512, validation_split=0.2)\n",
    "\n",
    "# 6. Evaluate the model\n",
    "results = model.evaluate(test_data, test_labels)\n",
    "print(f\"Test Loss: {results[0]:.4f}\")\n",
    "print(f\"Test Accuracy: {results[1]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eab1ee3b-c2d0-4a7e-a0da-6f492ef36a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
      "\u001b[1m1641221/1641221\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n",
      "Prediction Score: 0.6976\n",
      "Sentiment: Positive ðŸ˜€\n"
     ]
    }
   ],
   "source": [
    "# 7. Test with a new review (optional)\n",
    "\n",
    "# Load word index mapping (word -> int)\n",
    "word_index = keras.datasets.imdb.get_word_index()\n",
    "\n",
    "# Function to encode a review (text -> integers)\n",
    "def encode_review(text):\n",
    "    words = text.lower().split()\n",
    "    encoded = [word_index.get(word, 2) for word in words]  # 2 = unknown words\n",
    "    return keras.preprocessing.sequence.pad_sequences([encoded], maxlen=256)\n",
    "\n",
    "# Example custom review\n",
    "sample_review = \"This movie was absolutely wonderful, great acting and story\"\n",
    "encoded_review = encode_review(sample_review)\n",
    "\n",
    "# Predict\n",
    "prediction = model.predict(encoded_review)\n",
    "print(f\"Prediction Score: {prediction[0][0]:.4f}\")\n",
    "\n",
    "if prediction[0][0] > 0.5:\n",
    "    print(\"Sentiment: Positive ðŸ˜€\")\n",
    "else:\n",
    "    print(\"Sentiment: Negative ðŸ˜ž\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
